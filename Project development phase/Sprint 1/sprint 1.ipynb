{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING THE NECESSARY LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONNECTING DATASET WITH CODE\n",
    "import os, types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='h1FPcWHr9FB2AeQQJsj776i_eopbZe6FfyOzYsA2PF2j',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3.private.us.cloud-object-storage.appdomain.cloud')\n",
    "\n",
    "bucket = 'naturaldisasterintensityanalysisa-donotdelete-pr-pwiuxy2i5hirv2'\n",
    "object_key = 'Cyclone_Wildfire_Flood_Earthquake_Database.zip'\n",
    "\n",
    "streaming_body_2 = cos_client.get_object(Bucket=bucket, Key=object_key)['Body']\n",
    "\n",
    "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
    "# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
    "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# pandas documentation: http://pandas.pydata.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACTING THE DATASET USING BytesIO unzip function\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "unzip=zipfile.ZipFile(BytesIO(streaming_body_2.read()),'r')\n",
    "file_paths=unzip.namelist()\n",
    "for path in file_paths:\n",
    "    unzip.extract(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mCyclone_Wildfire_Flood_Earthquake_Database\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/wsuser/work'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=r'/home/wsuser/work/Cyclone_Wildfire_Flood_Earthquake_Database'\n",
    "#CONFIGURING THE ImageDataGenerator CLASS\n",
    "train_datagen=ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readme.txt\n",
      "Earthquake\n",
      "Cyclone\n",
      "Wildfire\n",
      "Flood\n"
     ]
    }
   ],
   "source": [
    "#LISTING OUT THE DISASTER CLASSES\n",
    "for i in os.listdir(dir):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.path.join(dir,'readme.txt')\n",
    "os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earthquake\n",
      "Cyclone\n",
      "Wildfire\n",
      "Flood\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir(dir):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting split_folders\n",
      "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
      "Installing collected packages: split-folders\n",
      "Successfully installed split-folders-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install split_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 4428 files [00:02, 1678.84 files/s]\n"
     ]
    }
   ],
   "source": [
    "#SPLIT THE DATASET INTO TRAINING, TESTING AND VALIDATION DATA\n",
    "import splitfolders\n",
    "splitfolders.ratio(dir,output=\"dataset\",seed=42,ratio=(.7,.2,.1),group_prefix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earthquake\n",
      "Cyclone\n",
      "Wildfire\n",
      "Flood\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir(dir):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir1=r'/home/wsuser/work/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n",
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for i in os.listdir(dir1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3097 images belonging to 4 classes.\n",
      "Found 884 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#Apply ImageDataGenerator Functionality to Trainset and Testset(Validation Set)\n",
    "x_train = train_datagen.flow_from_directory(r\"/home/wsuser/work/dataset/train\",\n",
    "                                            target_size=(224,224),\n",
    "                                            batch_size=5,\n",
    "                                            color_mode='rgb',\n",
    "                                            class_mode='categorical')\n",
    "x_val=test_datagen.flow_from_directory(r\"/home/wsuser/work/dataset/val\",\n",
    "                                        target_size=(224,224),\n",
    "                                        batch_size=5,\n",
    "                                        color_mode='rgb',\n",
    "                                        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 447 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#70% of images goes to training, #20% of the total to validation, and #10% remaining goes to testing data\n",
    "x_test=test_datagen.flow_from_directory(r\"/home/wsuser/work/dataset/test\")\n",
    "                                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
